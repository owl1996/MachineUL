{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from utils import loaders_by_classes, filter_loaders\n",
    "from classNet import ConvNet\n",
    "from target import fast_normalization_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./models/all_class.pth', weights_only=False)\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0,), (1,))  # Normalize images\n",
    "])\n",
    "\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = os.cpu_count()\n",
    "test_loaders = loaders_by_classes(test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = '0 - zero'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des gradients pour Loss_1\n",
    "\n",
    "filtered_loader = filter_loaders(test_loaders, class_name, batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlast_layer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'last_layer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_layer_grad(model, layer, input, target, criterion):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the criterion with respect to the input of a specified layer.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        layer (torch.nn.Module): The layer at which to get the gradient (e.g., the last layer).\n",
    "        input (torch.Tensor): The input data for the model.\n",
    "        target (torch.Tensor): The true target for the input data.\n",
    "        criterion (callable): The loss function (e.g., torch.nn.CrossEntropyLoss).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The gradient of the loss with respect to the input of the specified layer.\n",
    "    \"\"\"\n",
    "    layer_input_grad = None\n",
    "\n",
    "    # Hook to capture the gradient of the input at the specified layer\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        nonlocal layer_input_grad\n",
    "        layer_input_grad = grad_input[0]  # The gradient w.r.t. the input activations (128 dimensions)\n",
    "        \n",
    "    # Register the backward hook on the layer\n",
    "    hook = layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    # Perform a forward pass and calculate the loss\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    # Perform a backward pass to calculate gradients\n",
    "    model.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Remove the hook\n",
    "    hook.remove()\n",
    "\n",
    "    if layer_input_grad is None:\n",
    "        raise RuntimeError(\"Layer input gradient is None; ensure the layer has `requires_grad=True`.\")\n",
    "\n",
    "    # Return the gradient of the layer's input (128 dimensions)\n",
    "    return layer_input_grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = next(iter(filtered_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = input.to(device), label.to(device)\n",
    "last_layer_name, last_layer = list(model.named_children())[-1]\n",
    "get_layer_grad(model, last_layer, input, label, torch.nn.CrossEntropyLoss()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=10, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, layer = list(model.named_children())[-1]\n",
    "layer.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Layer input gradient is None; ensure the layer has `requires_grad=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Return the gradient of the layer's input\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_input_grad\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m---> 64\u001b[0m \u001b[43mget_target_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m, in \u001b[0;36mget_target_grad\u001b[0;34m(model, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m bwd_hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_input_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer input gradient is None; ensure the layer has `requires_grad=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Return the gradient of the layer's input\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layer_input_grad\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Layer input gradient is None; ensure the layer has `requires_grad=True`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_target_grad(model, input):\n",
    "    class_num = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the gradient of the loss with respect to the input of the last layer.\n",
    "    The target label is dynamically generated from the output of the model after the last layer.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        input (torch.Tensor): The input data for the model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The gradient of the loss with respect to the input of the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the last layer of the model\n",
    "    _, layer = list(model.named_children())[-1]\n",
    "    layer_input_grad = None\n",
    "\n",
    "    ll_output = None\n",
    "\n",
    "    # Hook to capture the output of the last layer\n",
    "    def forward_hook_last(module, inp, output):\n",
    "        nonlocal ll_output\n",
    "        ll_output = output\n",
    "\n",
    "    # Hook to capture the gradient of the input at the last layer\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        nonlocal layer_input_grad\n",
    "        layer_input_grad = grad_input[0]  # Gradient w.r.t. the input activations\n",
    "\n",
    "    # Register the forward hook on the last layer to capture its output\n",
    "    fwd_hook = layer.register_forward_hook(forward_hook_last)\n",
    "    bwd_hook = layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    # Perform a forward pass to calculate the model output and trigger hooks\n",
    "    model.zero_grad()\n",
    "    _ = model(input)\n",
    "\n",
    "    # Dynamically generate the target based on the last layer output\n",
    "    output_excluded = torch.cat([ll_output[:, :class_num], ll_output[:, class_num+1:]], dim=1)\n",
    "    mean_excluded = torch.mean(output_excluded, dim=1, keepdim=True)\n",
    "    target = output_excluded - mean_excluded\n",
    "\n",
    "    # Calculate the loss using the dynamically generated target\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss = criterion(output_excluded, target)  # Convert target to float if necessary for MSELoss\n",
    "\n",
    "    # Perform a backward pass to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Remove hooks after backward pass\n",
    "    fwd_hook.remove()\n",
    "    bwd_hook.remove()\n",
    "\n",
    "    if layer_input_grad is None:\n",
    "        raise RuntimeError(\"Layer input gradient is None; ensure the layer has `requires_grad=True`.\")\n",
    "\n",
    "    # Return the gradient of the layer's input\n",
    "    return layer_input_grad.clone()\n",
    "\n",
    "get_target_grad(model, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Layer input gradient is None; ensure the layer has `requires_grad=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_target_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m, in \u001b[0;36mget_target_grad\u001b[0;34m(model, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m bwd_hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_input_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer input gradient is None; ensure the layer has `requires_grad=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Return the gradient of the layer's input\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layer_input_grad\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Layer input gradient is None; ensure the layer has `requires_grad=True`."
     ]
    }
   ],
   "source": [
    "get_target_grad(model, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_1(model, input, target):\n",
    "    output = model(input)\n",
    "    loss = Loss_1(output, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_function_2(model, input, target):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error (MSE) between the output of a specified layer and a target.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        input (torch.Tensor): The input data for the model.\n",
    "        target (torch.Tensor): The target values for the layer output.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed MSE loss.\n",
    "    \"\"\"\n",
    "    _, layer = list(model.named_children())[-1]\n",
    "\n",
    "    layer_output = None\n",
    "\n",
    "    # Define a forward hook to capture the output of the specified layer\n",
    "    def forward_hook(module, inp, output):\n",
    "        nonlocal layer_output\n",
    "        layer_output = output\n",
    "        \n",
    "    # Register the hook on the layer\n",
    "    hook = layer.register_forward_hook(forward_hook)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    model(input)\n",
    "\n",
    "    # Remove the hook after forward pass\n",
    "    hook.remove()\n",
    "\n",
    "    if layer_output is None:\n",
    "        raise RuntimeError(\"Layer output is None; ensure the layer has been reached during forward pass.\")\n",
    "\n",
    "    # Calculate MSE loss between the layer output and the target\n",
    "    loss = torch.mean(layer_output, target)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = '0 - zero'\n",
    "target, output = fast_normalization_method(class_name, model, test_loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_loss = torch.nn.MSELoss()\n",
    "loss = L2_loss(output, target)\n",
    "loss.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/d5wm_d6x5zv68h4w18r3246h4xmqkj/T/ipykernel_10768/3355736690.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1728928983675/work/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  loss.grad\n"
     ]
    }
   ],
   "source": [
    "loss.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
